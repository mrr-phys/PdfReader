{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Parsing the pdf resume into a database with the following fields:\n",
    "\n",
    "•\tFirst Name \n",
    "\n",
    "•\tLast Name\n",
    "\n",
    "•\tEducation\n",
    "\n",
    "•\tDate of Degree(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Python packages\n",
    "\n",
    "import glob  \n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from tika import parser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "## Python packages for Database- you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Info Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_df(resume, column_headings, personalId):\n",
    "    \n",
    "    \"\"\"Create a Pandas DataFrame for the user infromation in the resume.\n",
    "        input\n",
    "    ------------\n",
    "    resume: all of the text Tika parses from the resume \n",
    "    column_headings: the list of column headings for the DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    #Tokenizing and filtering the resume off stopwords and punctuations \n",
    "    tokens = word_tokenize(resume)\n",
    "    #storing the cleaned resume\n",
    "    filtered = [w for w in tokens if w not in stop_words and w not in string.punctuation]\n",
    "    #get the name from the resume\n",
    "    first_name  = str(filtered[2])\n",
    "    last_name = str(filtered[5])\n",
    "    print \"Name : \" + first_name + ' ' +last_name\n",
    "\n",
    "    # Using regular expressions to extract email address\n",
    "    email = \"\"\n",
    "    match_email = re.search(r'[\\w\\.-]+@[\\w\\.-]+', resume) \n",
    "    email = match_email.group(0)\n",
    "    print email\n",
    "    \n",
    "    personal_info = [[personalId, first_name, last_name, email]]\n",
    "    \n",
    "    df = pd.DataFrame(personal_info, columns=column_headings)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def education_df(resume, pattern_wi_content, pattern_wi_line, column_headings, personalId):\n",
    "    \n",
    "    \"\"\" Create a Pandas DataFrame for the Education section in the resume.\n",
    "\n",
    "        input\n",
    "    ------------\n",
    "        \n",
    "    resume: all of the text Tika parses from the resume \n",
    "    content_pattern: a pattern that identifies the set of lines that will become rows in the DataFrame\n",
    "    line_pattern: a pattern within the content_pattern\n",
    "    column_headings: the list of column headings for the DataFrame \"\"\"\n",
    "\n",
    "    # Grab all of the lines of text that match the pattern in content_pattern\n",
    "    matched = re.search(pattern_wi_content, resume, re.DOTALL)\n",
    "    # group(1): only keep the lines between the parentheses in the pattern\n",
    "    matched = matched.group(1)\n",
    "    # Split on newlines to create a sequence of strings\n",
    "    matched = matched.split('\\n')\n",
    "    # Iterate over each line\n",
    "    \n",
    "    lines_list = []\n",
    "    for item in matched:\n",
    "        \n",
    "        if bool(re.search(r'\\d', item)):\n",
    "                \n",
    "                line_match = re.search(pattern_wi_line, item)\n",
    "\n",
    "                from_year = line_match.group(1)\n",
    "                to_year = line_match.group(3)\n",
    "                school = line_match.group(4)\n",
    "                state = line_match.group(5)\n",
    "                \n",
    "                lines_list.append([personalId, school, state, from_year, to_year])\n",
    "                \n",
    "                       \n",
    "    # Convert the list of lists into a Pandas DataFrame and specify the column headings\n",
    "    df = pd.DataFrame(lines_list, columns=column_headings)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name : Edward Smith\n",
      "ned-smith@kellogg.northwestern.edu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Smith</td>\n",
       "      <td>ned-smith@kellogg.northwestern.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID First Name Last Name                               Email\n",
       "0   1     Edward     Smith  ned-smith@kellogg.northwestern.edu"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the Education table in the resume, grab all of the lines between Education and RESEARCH INTERESTS\n",
    "education_pattern = r'EDUCATION(.*?)\\nRESEARCH INTERESTS'\n",
    "\n",
    "# For the education, grab the college name, years and state\n",
    "college_pattern = r'([\\d]{4}).*?([\\S]{6})?\\s+([\\d]{4})?\\s+([\\S\\s]+)\\s+(\\w+,\\s\\w+)'\n",
    "\n",
    "\n",
    "# Column headings for the Education DataFrames\n",
    "college_columns = ['ID', 'School', 'State', 'From_year', 'To_year']\n",
    "user_columns = ['ID', 'First Name', 'Last Name', 'Email']\n",
    "\n",
    "# Iterate over all PDF files in the folder and process each one in turn\n",
    "for input_file in glob.glob(os.path.join('/Users/mohammad/Desktop/Vanguard/Resume_Folder', '*.pdf')):\n",
    "    # Grab the PDF's file name\n",
    "    filename = os.path.basename(input_file)\n",
    "    \n",
    "    personalId=1\n",
    "\n",
    "    # Use Tika to parse the PDF\n",
    "    parsedPDF = parser.from_file(input_file)\n",
    "    # Extract the text content from the parsed PDF\n",
    "    resume = parsedPDF[\"content\"]\n",
    "    # Convert double newlines into single newlines\n",
    "    resume = resume.replace('\\n\\n', '\\n')\n",
    "\n",
    "    # Create a Pandas DataFrame from the lines of text in the Education table\n",
    "    education_df = education_df(resume, education_pattern, college_pattern, college_columns, personalId)\n",
    "    user_df = user_df(resume, user_columns, personalId)\n",
    "    \n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Transfer Extracted Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In Python: Have chosen username for my computer (CHANGE IT BELOW). \n",
    "dbname = 'VANGUARD_DB'\n",
    "username = 'mrr-phys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://mrr-phys@localhost/VANGUARD_DB\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, I'm using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## transfer the datasets into database \n",
    "user_df.to_sql('user_table', engine, if_exists='replace')\n",
    "education_df.to_sql('education_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>School</th>\n",
       "      <th>State</th>\n",
       "      <th>From_year</th>\n",
       "      <th>To_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Chicago, Booth School of Business</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yale University     New</td>\n",
       "      <td>Haven, CT</td>\n",
       "      <td>1999</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Capital University of Economics and Business</td>\n",
       "      <td>Beijing, China</td>\n",
       "      <td>2001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ID                                            School  \\\n",
       "0      0   1  University of Chicago, Booth School of Business    \n",
       "1      1   1                           Yale University     New   \n",
       "2      2   1     Capital University of Economics and Business    \n",
       "\n",
       "            State From_year To_year  \n",
       "0     Chicago, IL      2005    2010  \n",
       "1       Haven, CT      1999    2003  \n",
       "2  Beijing, China      2001    None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "\n",
    "# query:\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM education_table;\n",
    "\"\"\"\n",
    "user_data_from_sql = pd.read_sql_query(sql_query,con)\n",
    "\n",
    "user_data_from_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** In conclusion, I parsed the pdf with Tika module and extracted the education information and first/last name. I stored these data in PostGreSQL as two tables of user and education. Finding first and last name could be approached by using name entity recognition/analyzing the \"chuncked\" text data. Initially I followed this path but it was still not reliable enough to get the initial names correctlly for this case. I believe the information in the email address and metadata of the pdf file would be useful for future studies to detect the first/last name more accurately. Besides Tika, there is another PDFMiner parser that is a powerful module since it gives great access to interpreters and layout of pdf files, I will consider using it for the future analysis. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
